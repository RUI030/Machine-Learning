{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **0. ToolBox**"
      ],
      "metadata": {
        "id": "PRDkSVKei9K4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thop\n",
        "!pip install facenet-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o52o4_7Xi_E8",
        "outputId": "3be2cd43-30d8-4ac0-968a-15d0afb32340"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.3.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n",
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.25.2)\n",
            "Collecting Pillow<10.3.0,>=10.2.0 (from facenet-pytorch)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.31.0)\n",
            "Collecting torch<2.3.0,>=2.2.0 (from facenet-pytorch)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m474.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision<0.18.0,>=0.17.0 (from facenet-pytorch)\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2024.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m512.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n",
            "Installing collected packages: triton, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, facenet-pytorch\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 10.3.0\n",
            "    Uninstalling pillow-10.3.0:\n",
            "      Successfully uninstalled pillow-10.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cpu\n",
            "    Uninstalling torch-2.3.0+cpu:\n",
            "      Successfully uninstalled torch-2.3.0+cpu\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.0+cpu\n",
            "    Uninstalling torchvision-0.18.0+cpu:\n",
            "      Successfully uninstalled torchvision-0.18.0+cpu\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cpu requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.2.0 facenet-pytorch-2.6.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchvision-0.17.2 triton-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "d1c056d7106141b597e76e0d4da52be1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import cv2\n",
        "import os\n",
        "from facenet_pytorch import MTCNN\n",
        "\n",
        "#\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import Resize\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "#\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "#\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#\n",
        "from thop import profile\n",
        "from thop import clever_format\n",
        "#\n",
        "import numpy as np\n",
        "from skimage.feature import local_binary_pattern"
      ],
      "metadata": {
        "id": "poK5uCApi_dd"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Filepath**"
      ],
      "metadata": {
        "id": "DUjj3qt5jBNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File Path\n",
        "fp      = \"/content/drive/MyDrive/Class/ML\"\n",
        "fp_ds   = fp+'/dataset'\n",
        "fp_face = fp+'/face'\n",
        "fp_body = fp+'/body'\n",
        "fp_HOG  = fp+'/hog'\n",
        "fp_LBP  = fp+'/lbp'"
      ],
      "metadata": {
        "id": "0WKWDZI7jEU1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Preprocessing / Augmentation**"
      ],
      "metadata": {
        "id": "cL2Eu8kNjGPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crop Face (MTCNN)\n",
        "def crop_faces(folder_path, save_cropped=False, save_path=fp_face, subdir='/train'):\n",
        "    input_directory = folder_path + subdir\n",
        "    output_directory_faces = save_path + '2' + subdir\n",
        "    output_directory_lbp = save_path + '3' + subdir\n",
        "\n",
        "    # Create the output directories if they do not exist\n",
        "    os.makedirs(output_directory_faces, exist_ok=True)\n",
        "    os.makedirs(output_directory_lbp, exist_ok=True)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    mtcnn = MTCNN(keep_all=True, device=device)\n",
        "\n",
        "    for filename in os.listdir(input_directory):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(input_directory, filename)\n",
        "            image = Image.open(image_path)\n",
        "            image_rgb = np.array(image)\n",
        "\n",
        "            boxes, probs = mtcnn.detect(image)\n",
        "\n",
        "            if boxes is not None:\n",
        "                for i, (box, prob) in enumerate(zip(boxes, probs)):\n",
        "                    if prob > 0.5:\n",
        "                        x_min, y_min, x_max, y_max = map(int, box)\n",
        "                        # Validate box dimensions to ensure it's not empty\n",
        "                        if x_min >= x_max or y_min >= y_max:\n",
        "                            print(f\"Invalid box dimensions for {filename}: ({x_min}, {y_min}, {x_max}, {y_max})\")\n",
        "                            continue\n",
        "\n",
        "                        if x_min < 0:\n",
        "                            x_min = 0\n",
        "                        if y_min < 0:\n",
        "                            y_min = 0\n",
        "                        if x_max > image_rgb.shape[1]:\n",
        "                            x_max = image_rgb.shape[1]\n",
        "                        if y_max > image_rgb.shape[0]:\n",
        "                            y_max = image_rgb.shape[0]\n",
        "\n",
        "                        face_image = image_rgb[y_min:y_max, x_min:x_max]\n",
        "                        if face_image.size == 0:\n",
        "                            print(f\"No data in cropped face for {filename}.\")\n",
        "                            continue\n",
        "\n",
        "                        gray_face = cv2.cvtColor(face_image, cv2.COLOR_RGB2GRAY)\n",
        "                        equalized_face = cv2.equalizeHist(gray_face)\n",
        "\n",
        "                        lbp = local_binary_pattern(equalized_face, 8, 1, method='uniform')\n",
        "                        lbp_normalized = cv2.normalize(lbp, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "                        # Save cropped face and LBP images if requested\n",
        "                        if save_cropped:\n",
        "                            face_output_path = os.path.join(output_directory_faces, f'{filename[:-4]}_{i}.jpg')\n",
        "                            lbp_output_path = os.path.join(output_directory_lbp, f'{filename[:-4]}_{i}.jpg')\n",
        "                            Image.fromarray(face_image).save(face_output_path)\n",
        "                            cv2.imwrite(lbp_output_path, lbp_normalized)"
      ],
      "metadata": {
        "id": "h_Xvccw4jK9b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crop Face & Save\n",
        "crop_faces(fp_ds, save_cropped=True, subdir='/train/adults')\n",
        "crop_faces(fp_ds, save_cropped=True, subdir='/train/children')\n",
        "crop_faces(fp_ds, save_cropped=True, subdir='/test/adults')\n",
        "crop_faces(fp_ds, save_cropped=True, subdir='/test/children')"
      ],
      "metadata": {
        "id": "-K51GUUejNKC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Model**\n",
        "## **a. DenseNet**"
      ],
      "metadata": {
        "id": "_0vyWF7KjOib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DenseNet\n",
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, input_features, growth_rate, bn_size):\n",
        "        super(DenseLayer, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(input_features)\n",
        "        self.conv1 = nn.Conv2d(input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
        "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = torch.cat([x, out], 1)    # concatenate all previous feature maps and the output feature maps of current layer\n",
        "        return out\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, num_layers, input_features, bn_size, growth_rate):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(DenseLayer(input_features + i * growth_rate, growth_rate, bn_size))   # #feature_in = #input_features + i*growth_rate, #feature_out = growth_rate\n",
        "        self.layers = nn.Sequential(*layers)    # * : unpacking the list \"layers\" and combine them with nn.Sequential\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class TransitionLayer(nn.Module):\n",
        "    def __init__(self, input_features, output_features):\n",
        "        super(TransitionLayer, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(input_features)\n",
        "        self.conv = nn.Conv2d(input_features, output_features, kernel_size=1, stride=1, bias=False)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pool(self.conv(F.relu(self.bn(x))))\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            growth_rate=12,\n",
        "            block_config=(6, 12, 24, 16),\n",
        "            compression=0.5,\n",
        "            num_init_features=64,\n",
        "            bn_size=4,\n",
        "            num_classes=2\n",
        "        ):\n",
        "        super(DenseNet, self).__init__()\n",
        "        # Initial convolution\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(num_init_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )   # #feature_map: 3 -> 64\n",
        "\n",
        "        # Dense Blocks\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = DenseBlock(num_layers, num_features, bn_size, growth_rate)  # #layer = block_config[i], #features = #init_features + i* num_layers * growth_rate\n",
        "            self.features.add_module(f\"denseblock{i+1}\", block)\n",
        "            num_features = num_features + num_layers * growth_rate              # update num_features for next block\n",
        "            # add transition layer if it's not the last block\n",
        "            if i < len(block_config) - 1:\n",
        "                trans = TransitionLayer(num_features, int(num_features * compression))\n",
        "                self.features.add_module(f\"transition{i+1}\", trans)\n",
        "                num_features = int(num_features * compression)\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "-Z6uH1M-jTZ1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **b. FaceResNet**"
      ],
      "metadata": {
        "id": "KItw9yHrjl9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=2):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def ResNet18(num_classes=2):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)"
      ],
      "metadata": {
        "id": "_FZMn_fVjulQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **c. Ensemble**"
      ],
      "metadata": {
        "id": "UE47eL1ljwZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerNN(nn.Module):\n",
        "    def __init__(self, input_size, nLayers, nNeurons, output_size):\n",
        "        super(MultiLayerNN, self).__init__()\n",
        "        self.L_in = nn.Linear(input_size, nNeurons)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(nLayers):\n",
        "            self.layers.append(nn.Linear(nNeurons, nNeurons))\n",
        "        self.L_out = nn.Linear(nNeurons, output_size)\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1) #Data x 28 x 28 -> #Data x 784\n",
        "        x = torch.relu(self.L_in(x))\n",
        "        for layer in self.layers:\n",
        "            x = torch.relu(layer(x))\n",
        "        x = self.L_out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lMH1RUavj23n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Train**"
      ],
      "metadata": {
        "id": "bqw2j5_mj8jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "vADTKy6Ck2ko"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def TrainModel(model, trainloader, testloader, epochs=10, lr=1e-3, save_model = True, model_name = \"DenseNet\"):\n",
        "    model_name = model_name + '.pth'\n",
        "    model.to(device)  # Move the model to the GPU\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()  # training mode\n",
        "    lossRec = [[],[]]\n",
        "    AccRec = [[],[]]\n",
        "    best_test_acc = 60\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move the data to the GPU\n",
        "            optimizer.zero_grad()  # Resets the gradients of all optimized `torch.Tensor`s.\n",
        "            outputs = model(inputs)  # forward propagation\n",
        "            loss = criterion(outputs, labels)  # calculate loss\n",
        "            loss.backward()  # backward propagation\n",
        "            optimizer.step()  # update weights\n",
        "\n",
        "        trainloss, trainacc = EvalModel(model, trainloader)\n",
        "        testloss, testacc = EvalModel(model, testloader)\n",
        "        lossRec[0].append(trainloss)\n",
        "        lossRec[1].append(testloss)\n",
        "        AccRec[0].append(trainacc)\n",
        "        AccRec[1].append(testacc)\n",
        "        print(f\"Epoch {ep+1} >>> Train Loss: {trainloss} Accuracy: {trainacc}% <<< Test Loss: {testloss} Accuracy: {testacc}%\")\n",
        "        scheduler.step()  # Adjust lr with scheduler\n",
        "\n",
        "        if save_model and testacc > best_test_acc:\n",
        "            best_test_acc = testacc\n",
        "            torch.save(model.state_dict(), fp+'/'+model_name)\n",
        "            print(\"Model saved!\")\n",
        "\n",
        "    return lossRec, AccRec\n",
        "\n",
        "# Testing\n",
        "def EvalModel(model, dataloader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()  # evaluation mode\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    _loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move the data to the GPU\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)  # get predicted result\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    loss_avg = _loss / len(dataloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return loss_avg, accuracy"
      ],
      "metadata": {
        "id": "sWXcx__UlJnp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "densenet1 = DenseNet(\n",
        "            growth_rate=12,\n",
        "            block_config=(6, 12, 24, 16),\n",
        "            compression=0.5,\n",
        "            num_init_features=64,\n",
        "            bn_size=4,\n",
        "            num_classes=2\n",
        "        )\n",
        "resnet1 = ResNet18()\n",
        "resnet2 = ResNet18()\n",
        "multilayer_nn = MultiLayerNN(5,4,20,2)"
      ],
      "metadata": {
        "id": "HDJL7C5zk4CT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DenseNet\n",
        "transform1 = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.2,0.2,0.2]),\n",
        "])\n",
        "trainset_ds = ImageFolder(fp_ds+'/train', transform=transform1)\n",
        "testset_ds = ImageFolder(fp_ds+'/test', transform=transform1)\n",
        "trainloader_ds = DataLoader(trainset_ds, batch_size=32, shuffle=True)\n",
        "testloader_ds = DataLoader(testset_ds, batch_size=32, shuffle=False)\n",
        "DenseNet_loss, DenseNet_acc = TrainModel(densenet1, trainloader_ds, testloader_ds, epochs=20, lr=1e-3, save_model = True, model_name = \"DenseNet\")"
      ],
      "metadata": {
        "id": "g-FzTUl1lK_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FaceResNet\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.2,0.2,0.2]),\n",
        "])\n",
        "trainset_face2 = ImageFolder(fp_face+'2/train', transform=transform2)\n",
        "testset_face2 = ImageFolder(fp_face+'2/test', transform=transform2)\n",
        "trainloader_face2 = DataLoader(trainset_face2, batch_size=32, shuffle=True)\n",
        "testloader_face2 = DataLoader(testset_face2, batch_size=32, shuffle=False)\n",
        "ResNet1_loss, ResNet1_acc = TrainModel(resnet1, trainloader_face2, testloader_face2, epochs=20, lr=1e-3, save_model = True, model_name = \"FaceResNet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9jgq7EFlV27",
        "outputId": "5664cdc7-dd38-4944-b961-bb0a64e982a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 >>> Train Loss: 0.6320363548066881 Accuracy: 61.91304347826087% <<< Test Loss: 0.6947482004761696 Accuracy: 59.66386554621849%\n",
            "Epoch 2 >>> Train Loss: 0.6708187576797273 Accuracy: 63.65217391304348% <<< Test Loss: 0.794597253203392 Accuracy: 57.983193277310924%\n",
            "Epoch 3 >>> Train Loss: 0.6127268473307291 Accuracy: 67.30434782608695% <<< Test Loss: 0.6602290868759155 Accuracy: 63.865546218487395%\n",
            "Model saved!\n",
            "Epoch 4 >>> Train Loss: 0.5731456014845107 Accuracy: 71.82608695652173% <<< Test Loss: 0.6260439530014992 Accuracy: 70.58823529411765%\n",
            "Model saved!\n",
            "Epoch 5 >>> Train Loss: 0.6536429458194308 Accuracy: 58.95652173913044% <<< Test Loss: 0.682550385594368 Accuracy: 52.94117647058823%\n",
            "Epoch 6 >>> Train Loss: 0.5555032210217582 Accuracy: 73.04347826086956% <<< Test Loss: 0.5848346874117851 Accuracy: 68.90756302521008%\n",
            "Epoch 7 >>> Train Loss: 0.5805496623118719 Accuracy: 68.17391304347827% <<< Test Loss: 0.6842668578028679 Accuracy: 59.66386554621849%\n",
            "Epoch 8 >>> Train Loss: 0.5229531129201254 Accuracy: 75.30434782608695% <<< Test Loss: 0.6826950684189796 Accuracy: 60.50420168067227%\n",
            "Epoch 9 >>> Train Loss: 0.49227011534902787 Accuracy: 75.30434782608695% <<< Test Loss: 0.669281542301178 Accuracy: 68.0672268907563%\n",
            "Epoch 10 >>> Train Loss: 0.5140857332282596 Accuracy: 75.82608695652173% <<< Test Loss: 0.7070841938257217 Accuracy: 63.02521008403362%\n",
            "Epoch 11 >>> Train Loss: 0.47913332283496857 Accuracy: 78.08695652173913% <<< Test Loss: 0.6624236702919006 Accuracy: 62.18487394957983%\n",
            "Epoch 12 >>> Train Loss: 0.4741751038365894 Accuracy: 80.69565217391305% <<< Test Loss: 0.5979533642530441 Accuracy: 68.90756302521008%\n",
            "Epoch 13 >>> Train Loss: 0.45728326671653324 Accuracy: 78.08695652173913% <<< Test Loss: 0.7904319316148758 Accuracy: 61.34453781512605%\n",
            "Epoch 14 >>> Train Loss: 0.3856344016061889 Accuracy: 82.6086956521739% <<< Test Loss: 0.6744724214076996 Accuracy: 65.54621848739495%\n",
            "Epoch 15 >>> Train Loss: 0.331478588283062 Accuracy: 86.08695652173913% <<< Test Loss: 0.6788145080208778 Accuracy: 65.54621848739495%\n",
            "Epoch 16 >>> Train Loss: 0.35289061235056984 Accuracy: 86.43478260869566% <<< Test Loss: 0.6626126319169998 Accuracy: 69.74789915966386%\n",
            "Epoch 17 >>> Train Loss: 0.3190866733590762 Accuracy: 85.91304347826087% <<< Test Loss: 0.8711501210927963 Accuracy: 64.70588235294117%\n",
            "Epoch 18 >>> Train Loss: 0.35378218938906986 Accuracy: 83.30434782608695% <<< Test Loss: 0.8530958965420723 Accuracy: 63.865546218487395%\n",
            "Epoch 19 >>> Train Loss: 0.21069645633300146 Accuracy: 92.17391304347827% <<< Test Loss: 0.9462812542915344 Accuracy: 68.0672268907563%\n",
            "Epoch 20 >>> Train Loss: 0.23755062081747585 Accuracy: 89.04347826086956% <<< Test Loss: 0.9213507175445557 Accuracy: 68.0672268907563%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FaceResNet\n",
        "transform3 = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.2,0.2,0.2]),\n",
        "])\n",
        "trainset_face3 = ImageFolder(fp_face+'3/train', transform=transform2)\n",
        "testset_face3 = ImageFolder(fp_face+'3/test', transform=transform2)\n",
        "trainloader_face3 = DataLoader(trainset_face3, batch_size=32, shuffle=True)\n",
        "testloader_face3 = DataLoader(testset_face3, batch_size=32, shuffle=False)\n",
        "ResNet2_loss, ResNet2_acc = TrainModel(resnet2, trainloader_face3, testloader_face3, epochs=20, lr=1e-3, save_model = True, model_name = \"FaceResNetLBP\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsBtNIr5gQDr",
        "outputId": "352ca9d4-747a-4645-fce1-9248116a6346"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 >>> Train Loss: 0.517496281199985 Accuracy: 73.04347826086956% <<< Test Loss: 0.5480726286768913 Accuracy: 76.47058823529412%\n",
            "Model saved!\n",
            "Epoch 2 >>> Train Loss: 0.5363896158006456 Accuracy: 74.08695652173913% <<< Test Loss: 0.5215811878442764 Accuracy: 73.94957983193277%\n",
            "Epoch 3 >>> Train Loss: 0.5439895259009467 Accuracy: 69.21739130434783% <<< Test Loss: 0.5605466216802597 Accuracy: 62.18487394957983%\n",
            "Epoch 4 >>> Train Loss: 0.5344344725211462 Accuracy: 71.1304347826087% <<< Test Loss: 0.5518599078059196 Accuracy: 70.58823529411765%\n",
            "Epoch 5 >>> Train Loss: 0.5541662590371238 Accuracy: 72.34782608695652% <<< Test Loss: 0.5861912369728088 Accuracy: 62.18487394957983%\n",
            "Epoch 6 >>> Train Loss: 0.5173406104246775 Accuracy: 77.3913043478261% <<< Test Loss: 0.5279541611671448 Accuracy: 78.15126050420169%\n",
            "Model saved!\n",
            "Epoch 7 >>> Train Loss: 0.4519716335667504 Accuracy: 79.30434782608695% <<< Test Loss: 0.5056336000561714 Accuracy: 73.10924369747899%\n",
            "Epoch 8 >>> Train Loss: 0.44025299780898625 Accuracy: 79.47826086956522% <<< Test Loss: 0.5115878358483315 Accuracy: 73.94957983193277%\n",
            "Epoch 9 >>> Train Loss: 0.39661093221770394 Accuracy: 80.52173913043478% <<< Test Loss: 0.5019531399011612 Accuracy: 73.94957983193277%\n",
            "Epoch 10 >>> Train Loss: 0.4587482462326686 Accuracy: 75.82608695652173% <<< Test Loss: 0.6135634146630764 Accuracy: 69.74789915966386%\n",
            "Epoch 11 >>> Train Loss: 0.33654054088724983 Accuracy: 83.30434782608695% <<< Test Loss: 0.5600512288510799 Accuracy: 73.94957983193277%\n",
            "Epoch 12 >>> Train Loss: 0.42640705075528884 Accuracy: 80.34782608695652% <<< Test Loss: 0.7385384105145931 Accuracy: 70.58823529411765%\n",
            "Epoch 13 >>> Train Loss: 0.36732741196950275 Accuracy: 82.6086956521739% <<< Test Loss: 0.7150496281683445 Accuracy: 68.90756302521008%\n",
            "Epoch 14 >>> Train Loss: 0.2737424700624413 Accuracy: 88.8695652173913% <<< Test Loss: 0.6028060466051102 Accuracy: 68.0672268907563%\n",
            "Epoch 15 >>> Train Loss: 0.23983200887838999 Accuracy: 92.0% <<< Test Loss: 0.5401351414620876 Accuracy: 73.94957983193277%\n",
            "Epoch 16 >>> Train Loss: 0.23024961766269472 Accuracy: 91.47826086956522% <<< Test Loss: 0.6831652820110321 Accuracy: 72.26890756302521%\n",
            "Epoch 17 >>> Train Loss: 0.1790178943839338 Accuracy: 94.08695652173913% <<< Test Loss: 0.564662016928196 Accuracy: 69.74789915966386%\n",
            "Epoch 18 >>> Train Loss: 0.08871674920535749 Accuracy: 97.73913043478261% <<< Test Loss: 0.7909781485795975 Accuracy: 67.22689075630252%\n",
            "Epoch 19 >>> Train Loss: 0.09702348853978845 Accuracy: 97.56521739130434% <<< Test Loss: 0.7438802719116211 Accuracy: 72.26890756302521%\n",
            "Epoch 20 >>> Train Loss: 0.06235130736604333 Accuracy: 97.91304347826087% <<< Test Loss: 1.057192012667656 Accuracy: 69.74789915966386%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ensemble**"
      ],
      "metadata": {
        "id": "qTu4k6tVuGdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ProbListDataLoader(fp, densenet_model, resnet_model, batch_size=32):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    mtcnn = MTCNN(keep_all=True, device=device)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.2, 0.2, 0.2])\n",
        "    ])\n",
        "\n",
        "    labeldict = {\n",
        "        \"adults\": 0,\n",
        "        \"children\": 1\n",
        "    }\n",
        "\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for subdir in os.listdir(fp):\n",
        "        subdir_path = os.path.join(fp, subdir)\n",
        "        if os.path.isdir(subdir_path):  # Ensure it is a directory\n",
        "            label = subdir  # Use the subdirectory name as the label\n",
        "            for filename in os.listdir(subdir_path):\n",
        "                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_path = os.path.join(subdir_path, filename)\n",
        "                    try:\n",
        "                        image = Image.open(image_path).convert('RGB')\n",
        "                        image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "                        # Predict with DenseNet\n",
        "                        with torch.no_grad():\n",
        "                            densenet_model.eval()\n",
        "                            densenet_output = densenet_model(image_tensor)\n",
        "                            densenet_prob = torch.nn.functional.softmax(densenet_output, dim=1).squeeze().tolist()[0:2]  # Take the first two probabilities\n",
        "\n",
        "                        # Process image for face detection and LBP feature extraction\n",
        "                        image_rgb = np.array(image)\n",
        "                        boxes, probs = mtcnn.detect(image)\n",
        "\n",
        "                        face_detected = 0\n",
        "                        if boxes is not None and probs is not None and len(probs) > 0:\n",
        "                            max_prob_index = np.argmax(probs)\n",
        "                            box = boxes[max_prob_index]\n",
        "                            x_min, y_min, x_max, y_max = map(int, box)\n",
        "                            x_min = max(x_min, 0)\n",
        "                            y_min = max(y_min, 0)\n",
        "                            x_max = min(x_max, image_rgb.shape[1])\n",
        "                            y_max = min(y_max, image_rgb.shape[0])\n",
        "\n",
        "                            face_image = image_rgb[y_min:y_max, x_min:x_max]\n",
        "                            if face_image.size > 0:\n",
        "                                gray_face = cv2.cvtColor(face_image, cv2.COLOR_RGB2GRAY)\n",
        "                                equalized_face = cv2.equalizeHist(gray_face)\n",
        "                                lbp = local_binary_pattern(equalized_face, 8, 1, method='uniform')\n",
        "                                lbp_normalized = cv2.normalize(lbp, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "                                lbp_rgb = np.stack((lbp_normalized,) * 3, axis=-1)  # Convert grayscale to RGB\n",
        "                                lbp_image = Image.fromarray(lbp_rgb)\n",
        "                                lbp_tensor = transform(lbp_image).unsqueeze(0).to(device)\n",
        "\n",
        "                                with torch.no_grad():\n",
        "                                    resnet_model.eval()\n",
        "                                    resnet_output = resnet_model(lbp_tensor)\n",
        "                                    resnet_prob = torch.nn.functional.softmax(resnet_output, dim=1).squeeze().tolist()[0:2]  # Take the first two probabilities\n",
        "\n",
        "                                features = densenet_prob + resnet_prob + [1]\n",
        "                                face_detected = 1\n",
        "                            else:\n",
        "                                features = densenet_prob + [0.5, 0.5] + [0]\n",
        "                        else:\n",
        "                            features = densenet_prob + [0.5, 0.5] + [0]\n",
        "\n",
        "                        features_list.append(features)\n",
        "                        labels_list.append(labeldict[label])  # Append the subdirectory name as the label\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    features_tensor = torch.tensor(features_list, dtype=torch.float32)\n",
        "    labels_tensor = torch.tensor(labels_list)  # This may need to be converted to numerical labels depending on your application\n",
        "\n",
        "    # Create TensorDataset and DataLoader\n",
        "    dataset = TensorDataset(features_tensor, labels_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "RX8HXV7_y0c5"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "densenet_model = DenseNet(\n",
        "            growth_rate=12,\n",
        "            block_config=(6, 12, 24, 16),\n",
        "            compression=0.5,\n",
        "            num_init_features=64,\n",
        "            bn_size=4,\n",
        "            num_classes=2\n",
        "        )\n",
        "resnet_model = ResNet18()\n",
        "densenet_model_path = os.path.join(fp, 'DenseNet.pth')\n",
        "resnet_model_path = os.path.join(fp, 'FaceResNetLBP.pth')\n",
        "densenet_state_dict = torch.load(densenet_model_path)\n",
        "resnet_state_dict = torch.load(resnet_model_path)\n",
        "densenet_model.load_state_dict(densenet_state_dict)\n",
        "resnet_model.load_state_dict(resnet_state_dict)\n",
        "\n",
        "trainloader_nn = ProbListDataLoader(fp_ds+'/train', densenet_model, resnet_model)\n",
        "testloader_nn = ProbListDataLoader(fp_ds+'/test', densenet_model, resnet_model)"
      ],
      "metadata": {
        "id": "peJVJtkZtVJ4"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train multilayer with p_list\n",
        "multilayer_nn = MultiLayerNN(5,4,20,2)\n",
        "# p list to dataloader\n",
        "NN_Loss, NN_Acc = TrainModel(multilayer_nn, trainloader_nn, testloader_nn, epochs=50, lr=1e-3, save_model = True, model_name = \"MultiLayerNN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePP5arvjp8n_",
        "outputId": "d80e52f3-3750-4330-d356-4d463c87ff76"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 >>> Train Loss: 0.6920076840453677 Accuracy: 50.0% <<< Test Loss: 0.6924751251935959 Accuracy: 50.0%\n",
            "Epoch 2 >>> Train Loss: 0.6898316641648611 Accuracy: 50.0% <<< Test Loss: 0.6906544417142868 Accuracy: 50.0%\n",
            "Epoch 3 >>> Train Loss: 0.6844618684715695 Accuracy: 50.0% <<< Test Loss: 0.6853564381599426 Accuracy: 50.0%\n",
            "Epoch 4 >>> Train Loss: 0.6696820590231154 Accuracy: 69.10714285714286% <<< Test Loss: 0.6735213696956635 Accuracy: 66.66666666666667%\n",
            "Model saved!\n",
            "Epoch 5 >>> Train Loss: 0.6305071115493774 Accuracy: 78.75% <<< Test Loss: 0.6407751590013504 Accuracy: 75.0%\n",
            "Model saved!\n",
            "Epoch 6 >>> Train Loss: 0.5460724648502138 Accuracy: 83.21428571428571% <<< Test Loss: 0.5708601474761963 Accuracy: 80.0%\n",
            "Model saved!\n",
            "Epoch 7 >>> Train Loss: 0.43930487003591323 Accuracy: 82.85714285714286% <<< Test Loss: 0.49282679706811905 Accuracy: 80.83333333333333%\n",
            "Model saved!\n",
            "Epoch 8 >>> Train Loss: 0.3769810199737549 Accuracy: 83.39285714285714% <<< Test Loss: 0.4661629945039749 Accuracy: 79.16666666666667%\n",
            "Epoch 9 >>> Train Loss: 0.36518873108757866 Accuracy: 83.39285714285714% <<< Test Loss: 0.4770626351237297 Accuracy: 79.16666666666667%\n",
            "Epoch 10 >>> Train Loss: 0.4115666796763738 Accuracy: 82.5% <<< Test Loss: 0.546748198568821 Accuracy: 77.5%\n",
            "Epoch 11 >>> Train Loss: 0.36485448231299716 Accuracy: 83.57142857142857% <<< Test Loss: 0.4665336050093174 Accuracy: 78.33333333333333%\n",
            "Epoch 12 >>> Train Loss: 0.36318086998330223 Accuracy: 83.57142857142857% <<< Test Loss: 0.4713823050260544 Accuracy: 80.0%\n",
            "Epoch 13 >>> Train Loss: 0.3636547889974382 Accuracy: 83.39285714285714% <<< Test Loss: 0.45397045463323593 Accuracy: 76.66666666666667%\n",
            "Epoch 14 >>> Train Loss: 0.36417723778221345 Accuracy: 83.92857142857143% <<< Test Loss: 0.4675091952085495 Accuracy: 80.83333333333333%\n",
            "Epoch 15 >>> Train Loss: 0.3658037723766433 Accuracy: 83.92857142857143% <<< Test Loss: 0.456687293946743 Accuracy: 80.0%\n",
            "Epoch 16 >>> Train Loss: 0.37062307447195053 Accuracy: 84.28571428571429% <<< Test Loss: 0.46680130809545517 Accuracy: 80.83333333333333%\n",
            "Epoch 17 >>> Train Loss: 0.38820136917961967 Accuracy: 83.57142857142857% <<< Test Loss: 0.49498601257801056 Accuracy: 78.33333333333333%\n",
            "Epoch 18 >>> Train Loss: 0.3734562968214353 Accuracy: 82.14285714285714% <<< Test Loss: 0.47387494146823883 Accuracy: 76.66666666666667%\n",
            "Epoch 19 >>> Train Loss: 0.36399118933412766 Accuracy: 84.10714285714286% <<< Test Loss: 0.45933856070041656 Accuracy: 79.16666666666667%\n",
            "Epoch 20 >>> Train Loss: 0.3622666282786263 Accuracy: 83.75% <<< Test Loss: 0.45983174443244934 Accuracy: 76.66666666666667%\n",
            "Epoch 21 >>> Train Loss: 0.36972266766760087 Accuracy: 84.28571428571429% <<< Test Loss: 0.4807063564658165 Accuracy: 80.83333333333333%\n",
            "Epoch 22 >>> Train Loss: 0.3635264022482766 Accuracy: 83.21428571428571% <<< Test Loss: 0.44778965041041374 Accuracy: 76.66666666666667%\n",
            "Epoch 23 >>> Train Loss: 0.3634351972076628 Accuracy: 83.75% <<< Test Loss: 0.4634174779057503 Accuracy: 76.66666666666667%\n",
            "Epoch 24 >>> Train Loss: 0.3611349140604337 Accuracy: 83.75% <<< Test Loss: 0.46371036022901535 Accuracy: 78.33333333333333%\n",
            "Epoch 25 >>> Train Loss: 0.36761753012736637 Accuracy: 83.75% <<< Test Loss: 0.46765026450157166 Accuracy: 77.5%\n",
            "Epoch 26 >>> Train Loss: 0.36096900949875516 Accuracy: 84.28571428571429% <<< Test Loss: 0.4714523181319237 Accuracy: 80.0%\n",
            "Epoch 27 >>> Train Loss: 0.3600270284546746 Accuracy: 84.10714285714286% <<< Test Loss: 0.4610648900270462 Accuracy: 80.0%\n",
            "Epoch 28 >>> Train Loss: 0.3727484941482544 Accuracy: 83.21428571428571% <<< Test Loss: 0.4566695764660835 Accuracy: 76.66666666666667%\n",
            "Epoch 29 >>> Train Loss: 0.36216342945893604 Accuracy: 84.28571428571429% <<< Test Loss: 0.47507505863904953 Accuracy: 80.83333333333333%\n",
            "Epoch 30 >>> Train Loss: 0.3603213065200382 Accuracy: 84.64285714285714% <<< Test Loss: 0.4813781604170799 Accuracy: 80.83333333333333%\n",
            "Epoch 31 >>> Train Loss: 0.3642137539055612 Accuracy: 84.28571428571429% <<< Test Loss: 0.4645002856850624 Accuracy: 80.83333333333333%\n",
            "Epoch 32 >>> Train Loss: 0.36822882874144447 Accuracy: 84.10714285714286% <<< Test Loss: 0.4643912836909294 Accuracy: 79.16666666666667%\n",
            "Epoch 33 >>> Train Loss: 0.3589031058881018 Accuracy: 83.39285714285714% <<< Test Loss: 0.4675784111022949 Accuracy: 76.66666666666667%\n",
            "Epoch 34 >>> Train Loss: 0.36202219873666763 Accuracy: 84.10714285714286% <<< Test Loss: 0.47599615901708603 Accuracy: 80.0%\n",
            "Epoch 35 >>> Train Loss: 0.3637568967209922 Accuracy: 84.28571428571429% <<< Test Loss: 0.45669031143188477 Accuracy: 80.0%\n",
            "Epoch 36 >>> Train Loss: 0.36921586924129063 Accuracy: 83.21428571428571% <<< Test Loss: 0.4579780772328377 Accuracy: 76.66666666666667%\n",
            "Epoch 37 >>> Train Loss: 0.357798480325275 Accuracy: 84.46428571428571% <<< Test Loss: 0.4670222997665405 Accuracy: 80.83333333333333%\n",
            "Epoch 38 >>> Train Loss: 0.37359018044339287 Accuracy: 83.03571428571429% <<< Test Loss: 0.4584057107567787 Accuracy: 75.83333333333333%\n",
            "Epoch 39 >>> Train Loss: 0.36294375525580513 Accuracy: 83.75% <<< Test Loss: 0.45625946298241615 Accuracy: 78.33333333333333%\n",
            "Epoch 40 >>> Train Loss: 0.3651943802833557 Accuracy: 83.92857142857143% <<< Test Loss: 0.45865823328495026 Accuracy: 78.33333333333333%\n",
            "Epoch 41 >>> Train Loss: 0.35852767278750736 Accuracy: 83.57142857142857% <<< Test Loss: 0.46308683604002 Accuracy: 76.66666666666667%\n",
            "Epoch 42 >>> Train Loss: 0.3621009323332045 Accuracy: 83.92857142857143% <<< Test Loss: 0.45980650186538696 Accuracy: 77.5%\n",
            "Epoch 43 >>> Train Loss: 0.3582984796828694 Accuracy: 84.28571428571429% <<< Test Loss: 0.4587352126836777 Accuracy: 80.83333333333333%\n",
            "Epoch 44 >>> Train Loss: 0.35832777702146107 Accuracy: 83.92857142857143% <<< Test Loss: 0.47877218574285507 Accuracy: 79.16666666666667%\n",
            "Epoch 45 >>> Train Loss: 0.36940738889906144 Accuracy: 83.21428571428571% <<< Test Loss: 0.46425674855709076 Accuracy: 76.66666666666667%\n",
            "Epoch 46 >>> Train Loss: 0.36140156785647076 Accuracy: 83.75% <<< Test Loss: 0.4694349691271782 Accuracy: 76.66666666666667%\n",
            "Epoch 47 >>> Train Loss: 0.365598373942905 Accuracy: 83.21428571428571% <<< Test Loss: 0.46023328602313995 Accuracy: 76.66666666666667%\n",
            "Epoch 48 >>> Train Loss: 0.365913074877527 Accuracy: 84.10714285714286% <<< Test Loss: 0.4796360805630684 Accuracy: 79.16666666666667%\n",
            "Epoch 49 >>> Train Loss: 0.36034138335122 Accuracy: 84.46428571428571% <<< Test Loss: 0.4940943196415901 Accuracy: 81.66666666666667%\n",
            "Model saved!\n",
            "Epoch 50 >>> Train Loss: 0.35952480385700863 Accuracy: 83.92857142857143% <<< Test Loss: 0.45241959393024445 Accuracy: 79.16666666666667%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.Test**"
      ],
      "metadata": {
        "id": "Owlam2A3lqJD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Fv5aJHwls-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Demo**"
      ],
      "metadata": {
        "id": "BPQtwfg21sI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Predict\n",
        "# =============================\n",
        "\n",
        "# =============================\n",
        "# Model info\n",
        "# =============================\n",
        "\n",
        "# PreProcessing\n",
        "\n",
        "# DenseNet\n",
        "\n",
        "# ResNet\n",
        "\n",
        "# SUM\n"
      ],
      "metadata": {
        "id": "ziVOZkCd1uo4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}